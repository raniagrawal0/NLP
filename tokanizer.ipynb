{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b41696d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c2d5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91630\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Directly stream the Hindi-Devanagari split\n",
    "hindi_stream = load_dataset(\n",
    "    \"ai4bharat/IndicCorpV2\",\n",
    "    \"indiccorp_v2\",\n",
    "    streaming=True,\n",
    "    split=\"hin_Deva\"  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3863ba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.34.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\91630\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91630\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d43c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# Example: Get first 5 entries from the iterable\n",
    "samples = list(islice(hindi_stream, 5))  # change 5 to any number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947bf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = samples[0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf95337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def hindi_tokenizer(text):\n",
    "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
    "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b'\n",
    "\n",
    "    # Save and replace URLs and emails\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    text = re.sub(url_pattern, '<URL>', text)\n",
    "\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token == '<URL>':\n",
    "            tokens.append(urls.pop(0))\n",
    "        elif token == '<EMAIL>':\n",
    "            tokens.append(emails.pop(0))\n",
    "        else:\n",
    "            split_tokens = re.findall(\n",
    "                r'[\\u0900-\\u097F]+|[a-zA-Z0-9]+|[ред.,!?;:()\\\"\\'\\-]|[^\\s]',\n",
    "                token\n",
    "            )\n",
    "            tokens.extend(split_tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def hindi_sentence_tokenizer(text):\n",
    "    sentence_end_pattern = r'(?<=[ред!?\\.])\\s+'\n",
    "    sentences = re.split(sentence_end_pattern, text.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def detokenize(tokens):\n",
    "    # Rebuild sentence with proper spacing logic\n",
    "    sentence = ''\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i > 0 and not re.match(r'[ред.,!?;:)\\]\\'\\\"]', token):\n",
    "            sentence += ' '\n",
    "        sentence += token\n",
    "    return sentence.strip()\n",
    "\n",
    "def hindi_corpus_statistics(text):\n",
    "    sentences = hindi_sentence_tokenizer(text)\n",
    "    all_tokens = []\n",
    "    reconstructed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = hindi_tokenizer(sentence)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "        # For checking reformation\n",
    "        reconstructed = detokenize(tokens)\n",
    "        reconstructed_sentences.append(reconstructed)\n",
    "\n",
    "    num_tokens = len(all_tokens)\n",
    "    unique_tokens = set(all_tokens)\n",
    "    total_chars = sum(len(token) for token in all_tokens)\n",
    "\n",
    "    word_tokens = [t for t in all_tokens if re.match(r'^[\\u0900-\\u097F\\w]+$', t)]\n",
    "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
    "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
    "\n",
    "    return {\n",
    "        'sentences': sentences,\n",
    "        'tokens': all_tokens,\n",
    "        'num_tokens': num_tokens,\n",
    "        'total_characters': total_chars,\n",
    "        'average_word_length': round(avg_word_length, 2),\n",
    "        'type_token_ratio': round(type_token_ratio, 3),\n",
    "        'reconstructed_sentences': reconstructed_sentences\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0f3048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "рд▓реЛрдЧреЛрдВ рдХреЛ рдмрд┐рд▓реЛрдВ рд╕рдВрдмрдВрдзреА рд╕реБрд╡рд┐рдзрд╛ рджреЗрдирд╛ рд╣реА рдЙрдирдХрд╛ рдХрд╛рдо  рдЗрдиреЗрд▓реЛ 1987 рдореЗрдВ рдЙрд╕ рд╡рдХреНрдд рдРрд╕реЗ рд╣реА рджреЛрд░рд╛рд╣реЗ рдкрд░ рдЦрдбрд╝реА рдереА, рдЬрдм рдкреВрд░реНрд╡ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдЕрдкрдиреЗ рдкреБрддреНрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЕрдкрдирд╛ рд░рд╛рдЬрдиреАрддрд┐рдХ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдШреЛрд╖рд┐рдд рдХрд┐рдпрд╛ рдерд╛ред\n",
      "рд╣рд╛рд▓рд╛рдВрдХрд┐ рддрдм рдкрд╛рд░реНрдЯреА рдкрд░ рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдХреЗ рдЪрд▓рддреЗ рдкрд╛рд░реНрдЯреА рдЯреВрдЯрдиреЗ рд╕реЗ рдмрдЪ рдЧрдИ рдереАред\n",
      "1989 рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдХреЗрдиреНрджреНрд░ рдХреА рд░рд╛рдЬрдиреАрддрд┐ рдореЗрдВ рд╕рдХреНрд░рд┐рдп рд╣реЛ рдЧрдП рдереЗ рдФрд░ рдЙрдирдХреЗ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рдмрдирдиреЗ рдХреЗ рдкрд╢реНрдЪрд╛рддреН рдЙрдирдХреЗ рддреАрди рдмреЗрдЯреЛрдВ рдЬрдЧрджреАрд╢ рд╕рд┐рдВрд╣, рд░рдгрдЬреАрдд рд╕рд┐рдВрд╣ рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдореЗрдВ рд╕реЗ рд░рдгрдЬреАрдд рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдХреЗ рдмреАрдЪ рд╣рд░рд┐рдпрд╛рдгрд╛ рдореЗрдВ рдЙрдирдХреА рд░рд╛рдЬрдиреАрддрд┐рдХ рд╡рд┐рд░рд╛рд╕рдд рдХреЛ рд▓реЗрдХрд░ рдЬрдВрдЧ рд╢реБрд░реВ рд╣реЛ рдЧрдИ рдереАред\n",
      "рдЙрди рдкрд░рд┐рд╕реНрдерд┐рддрд┐рдпреЛрдВ рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдХрдбрд╝рд╛ рдирд┐рд░реНрдгрдп рд▓реЗрддреЗ рд╣реБрдП рдкрд╛рд░реНрдЯреА рдХреА рдмрд╛рдЧрдбреЛрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЗ рд╣рд╡рд╛рд▓реЗ рдХрд░ рджреА рдереА, рдЬрд┐рд╕рдХреЗ рдмрд╛рдж рд░рдгрдЬреАрдд рдХреА рдмрдЧрд╛рд╡рдд рдХрд╛ рдЕрд╕рд░ рдкрд╛рд░реНрдЯреА, рд╕рдВрдЧрдарди рдФрд░ рдЙрдирдХреА рд╕рд░рдХрд╛рд░ рдкрд░ рднреА рдкрдбрд╝рд╛ рдерд╛ред\n",
      "рдЙрд╕ рд╕рдордп рд░рдгрдЬреАрдд рдХреА рдирд╛рд░рд╛рдЬрдЧреА рдХреЗ рдЪрд▓рддреЗ рдЙрдирдХреЗ рд╕рдорд░реНрдерди рдореЗрдВ рдХрдИ рдХреИрдмрд┐рдиреЗрдЯ рдордВрддреНрд░рд┐рдпреЛрдВ рдиреЗ рдЗрд╕реНрддреАрдлреЗ рджреЗ рджрд┐рдП рдереЗ рдХрд┐рдиреНрддреБ рддрдм рдкрд╛рд░реНрдЯреА рд╕реБрдкреНрд░реАрдореЛ рдЪреМ.\n",
      "рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рд╣рд░рд┐рдпрд╛рдгрд╛ рдХреА рдЬрдирддрд╛ рдкрд░ рдЗрддрдиреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдереА рдХрд┐ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдмрдирд╛рдиреЗ рдХреЗ рдЙрдирдХреЗ рдлреИрд╕рд▓реЗ рдХрд╛ рдЬрдирддрд╛ рдХреЗ рдмреАрдЪ рдХреЛрдИ рдЦрд╛рд╕ рд╡рд┐рд░реЛрдз рдирд╣реАрдВ рд╣реБрдЖ рдерд╛ рд▓реЗрдХрд┐рди рдЖрдЬ рд╕реНрдерд┐рддрд┐ рдмрд┐рд▓реНрдХреБрд▓ рд╡рд┐рдкрд░реАрдд рд╣реИред\n",
      "рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдкрд┐рдЫрд▓реЗ рдХрд╛рдлреА рд╕рдордп рд╕реЗ рдЬреЗрд▓ рдореЗрдВ рд╣реИрдВ рдФрд░ рдЬреЗрд▓ рдореЗрдВ рд░рд╣рддреЗ рдкрд╛рд░реНрдЯреА рдХреЗ рд╕рд╛рде-рд╕рд╛рде рдкрд░рд┐рд╡рд╛рд░ рдкрд░ рднреА рдЙрдирдХреА рдкрдХрдбрд╝ рдХрд╛рдлреА рдврд╝реАрд▓реА рд╣реЛ рдЧрдИ рд╣реИ, рдЗрд╕реА рдХрд╛рд░рдг рдЙрдирдореЗрдВ рдЕрдм рджреЗрд╡реАрд▓рд╛рд▓ рдЬреИрд╕рд╛ рд╡реЛ рд╕рд╛рдорд░реНрдереНрдп рдирдЬрд░ рдирд╣реАрдВ рдЖрддрд╛ рдХрд┐ рд╡реЗ рдЕрдкрдиреЗ рдлреИрд╕рд▓реЛрдВ рдХреЛ рдмрдЧреИрд░ рдХрд┐рд╕реА рдкреНрд░рддрд┐рд░реЛрдз рдХреЗ рд▓рд╛рдЧреВ рдХрд░рд╛ рд╕рдХреЗрдВред\n",
      "рдЬрд╣рд╛рдВ рдЖрдИ рдереА рддрдмрд╛рд╣реА рдЙрд╕ рдШрд╛рдЯреА рдХреНрд╖реЗрддреНрд░ рдореЗрдВ рдЦрддрд░рд╛ рдЬреНрдпрд╛рджрд╛\n",
      "\n",
      "Reconstructed Sentences:\n",
      "рд▓реЛрдЧреЛрдВ рдХреЛ рдмрд┐рд▓реЛрдВ рд╕рдВрдмрдВрдзреА рд╕реБрд╡рд┐рдзрд╛ рджреЗрдирд╛ рд╣реА рдЙрдирдХрд╛ рдХрд╛рдо рдЗрдиреЗрд▓реЛ 1987 рдореЗрдВ рдЙрд╕ рд╡рдХреНрдд рдРрд╕реЗ рд╣реА рджреЛрд░рд╛рд╣реЗ рдкрд░ рдЦрдбрд╝реА рдереА, рдЬрдм рдкреВрд░реНрд╡ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдЕрдкрдиреЗ рдкреБрддреНрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЕрдкрдирд╛ рд░рд╛рдЬрдиреАрддрд┐рдХ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдШреЛрд╖рд┐рдд рдХрд┐рдпрд╛ рдерд╛ред\n",
      "рд╣рд╛рд▓рд╛рдВрдХрд┐ рддрдм рдкрд╛рд░реНрдЯреА рдкрд░ рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдХреЗ рдЪрд▓рддреЗ рдкрд╛рд░реНрдЯреА рдЯреВрдЯрдиреЗ рд╕реЗ рдмрдЪ рдЧрдИ рдереАред\n",
      "1989 рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдХреЗрдиреНрджреНрд░ рдХреА рд░рд╛рдЬрдиреАрддрд┐ рдореЗрдВ рд╕рдХреНрд░рд┐рдп рд╣реЛ рдЧрдП рдереЗ рдФрд░ рдЙрдирдХреЗ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рдмрдирдиреЗ рдХреЗ рдкрд╢реНрдЪрд╛рддреН рдЙрдирдХреЗ рддреАрди рдмреЗрдЯреЛрдВ рдЬрдЧрджреАрд╢ рд╕рд┐рдВрд╣, рд░рдгрдЬреАрдд рд╕рд┐рдВрд╣ рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдореЗрдВ рд╕реЗ рд░рдгрдЬреАрдд рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдХреЗ рдмреАрдЪ рд╣рд░рд┐рдпрд╛рдгрд╛ рдореЗрдВ рдЙрдирдХреА рд░рд╛рдЬрдиреАрддрд┐рдХ рд╡рд┐рд░рд╛рд╕рдд рдХреЛ рд▓реЗрдХрд░ рдЬрдВрдЧ рд╢реБрд░реВ рд╣реЛ рдЧрдИ рдереАред\n",
      "рдЙрди рдкрд░рд┐рд╕реНрдерд┐рддрд┐рдпреЛрдВ рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдХрдбрд╝рд╛ рдирд┐рд░реНрдгрдп рд▓реЗрддреЗ рд╣реБрдП рдкрд╛рд░реНрдЯреА рдХреА рдмрд╛рдЧрдбреЛрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЗ рд╣рд╡рд╛рд▓реЗ рдХрд░ рджреА рдереА, рдЬрд┐рд╕рдХреЗ рдмрд╛рдж рд░рдгрдЬреАрдд рдХреА рдмрдЧрд╛рд╡рдд рдХрд╛ рдЕрд╕рд░ рдкрд╛рд░реНрдЯреА, рд╕рдВрдЧрдарди рдФрд░ рдЙрдирдХреА рд╕рд░рдХрд╛рд░ рдкрд░ рднреА рдкрдбрд╝рд╛ рдерд╛ред\n",
      "рдЙрд╕ рд╕рдордп рд░рдгрдЬреАрдд рдХреА рдирд╛рд░рд╛рдЬрдЧреА рдХреЗ рдЪрд▓рддреЗ рдЙрдирдХреЗ рд╕рдорд░реНрдерди рдореЗрдВ рдХрдИ рдХреИрдмрд┐рдиреЗрдЯ рдордВрддреНрд░рд┐рдпреЛрдВ рдиреЗ рдЗрд╕реНрддреАрдлреЗ рджреЗ рджрд┐рдП рдереЗ рдХрд┐рдиреНрддреБ рддрдм рдкрд╛рд░реНрдЯреА рд╕реБрдкреНрд░реАрдореЛ рдЪреМ.\n",
      "рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рд╣рд░рд┐рдпрд╛рдгрд╛ рдХреА рдЬрдирддрд╛ рдкрд░ рдЗрддрдиреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдереА рдХрд┐ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдмрдирд╛рдиреЗ рдХреЗ рдЙрдирдХреЗ рдлреИрд╕рд▓реЗ рдХрд╛ рдЬрдирддрд╛ рдХреЗ рдмреАрдЪ рдХреЛрдИ рдЦрд╛рд╕ рд╡рд┐рд░реЛрдз рдирд╣реАрдВ рд╣реБрдЖ рдерд╛ рд▓реЗрдХрд┐рди рдЖрдЬ рд╕реНрдерд┐рддрд┐ рдмрд┐рд▓реНрдХреБрд▓ рд╡рд┐рдкрд░реАрдд рд╣реИред\n",
      "рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдкрд┐рдЫрд▓реЗ рдХрд╛рдлреА рд╕рдордп рд╕реЗ рдЬреЗрд▓ рдореЗрдВ рд╣реИрдВ рдФрд░ рдЬреЗрд▓ рдореЗрдВ рд░рд╣рддреЗ рдкрд╛рд░реНрдЯреА рдХреЗ рд╕рд╛рде - рд╕рд╛рде рдкрд░рд┐рд╡рд╛рд░ рдкрд░ рднреА рдЙрдирдХреА рдкрдХрдбрд╝ рдХрд╛рдлреА рдврд╝реАрд▓реА рд╣реЛ рдЧрдИ рд╣реИ, рдЗрд╕реА рдХрд╛рд░рдг рдЙрдирдореЗрдВ рдЕрдм рджреЗрд╡реАрд▓рд╛рд▓ рдЬреИрд╕рд╛ рд╡реЛ рд╕рд╛рдорд░реНрдереНрдп рдирдЬрд░ рдирд╣реАрдВ рдЖрддрд╛ рдХрд┐ рд╡реЗ рдЕрдкрдиреЗ рдлреИрд╕рд▓реЛрдВ рдХреЛ рдмрдЧреИрд░ рдХрд┐рд╕реА рдкреНрд░рддрд┐рд░реЛрдз рдХреЗ рд▓рд╛рдЧреВ рдХрд░рд╛ рд╕рдХреЗрдВред\n",
      "рдЬрд╣рд╛рдВ рдЖрдИ рдереА рддрдмрд╛рд╣реА рдЙрд╕ рдШрд╛рдЯреА рдХреНрд╖реЗрддреНрд░ рдореЗрдВ рдЦрддрд░рд╛ рдЬреНрдпрд╛рджрд╛\n",
      "\n",
      "Tokens: ['рд▓реЛрдЧреЛрдВ', 'рдХреЛ', 'рдмрд┐рд▓реЛрдВ', 'рд╕рдВрдмрдВрдзреА', 'рд╕реБрд╡рд┐рдзрд╛', 'рджреЗрдирд╛', 'рд╣реА', 'рдЙрдирдХрд╛', 'рдХрд╛рдо', 'рдЗрдиреЗрд▓реЛ', '1987', 'рдореЗрдВ', 'рдЙрд╕', 'рд╡рдХреНрдд', 'рдРрд╕реЗ', 'рд╣реА', 'рджреЛрд░рд╛рд╣реЗ', 'рдкрд░', 'рдЦрдбрд╝реА', 'рдереА', ',', 'рдЬрдм', 'рдкреВрд░реНрд╡', 'рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдиреЗ', 'рдЕрдкрдиреЗ', 'рдкреБрддреНрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдХреЛ', 'рдЕрдкрдирд╛', 'рд░рд╛рдЬрдиреАрддрд┐рдХ', 'рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА', 'рдШреЛрд╖рд┐рдд', 'рдХрд┐рдпрд╛', 'рдерд╛ред', 'рд╣рд╛рд▓рд╛рдВрдХрд┐', 'рддрдм', 'рдкрд╛рд░реНрдЯреА', 'рдкрд░', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдХреА', 'рдордЬрдмреВрдд', 'рдкрдХрдбрд╝', 'рдХреЗ', 'рдЪрд▓рддреЗ', 'рдкрд╛рд░реНрдЯреА', 'рдЯреВрдЯрдиреЗ', 'рд╕реЗ', 'рдмрдЪ', 'рдЧрдИ', 'рдереАред', '1989', 'рдореЗрдВ', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдХреЗрдиреНрджреНрд░', 'рдХреА', 'рд░рд╛рдЬрдиреАрддрд┐', 'рдореЗрдВ', 'рд╕рдХреНрд░рд┐рдп', 'рд╣реЛ', 'рдЧрдП', 'рдереЗ', 'рдФрд░', 'рдЙрдирдХреЗ', 'рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА', 'рдмрдирдиреЗ', 'рдХреЗ', 'рдкрд╢реНрдЪрд╛рддреН', 'рдЙрдирдХреЗ', 'рддреАрди', 'рдмреЗрдЯреЛрдВ', 'рдЬрдЧрджреАрд╢', 'рд╕рд┐рдВрд╣', ',', 'рд░рдгрдЬреАрдд', 'рд╕рд┐рдВрд╣', 'рдФрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдореЗрдВ', 'рд╕реЗ', 'рд░рдгрдЬреАрдд', 'рдФрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдХреЗ', 'рдмреАрдЪ', 'рд╣рд░рд┐рдпрд╛рдгрд╛', 'рдореЗрдВ', 'рдЙрдирдХреА', 'рд░рд╛рдЬрдиреАрддрд┐рдХ', 'рд╡рд┐рд░рд╛рд╕рдд', 'рдХреЛ', 'рд▓реЗрдХрд░', 'рдЬрдВрдЧ', 'рд╢реБрд░реВ', 'рд╣реЛ', 'рдЧрдИ', 'рдереАред', 'рдЙрди', 'рдкрд░рд┐рд╕реНрдерд┐рддрд┐рдпреЛрдВ', 'рдореЗрдВ', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдиреЗ', 'рдХрдбрд╝рд╛', 'рдирд┐рд░реНрдгрдп', 'рд▓реЗрддреЗ', 'рд╣реБрдП', 'рдкрд╛рд░реНрдЯреА', 'рдХреА', 'рдмрд╛рдЧрдбреЛрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдХреЗ', 'рд╣рд╡рд╛рд▓реЗ', 'рдХрд░', 'рджреА', 'рдереА', ',', 'рдЬрд┐рд╕рдХреЗ', 'рдмрд╛рдж', 'рд░рдгрдЬреАрдд', 'рдХреА', 'рдмрдЧрд╛рд╡рдд', 'рдХрд╛', 'рдЕрд╕рд░', 'рдкрд╛рд░реНрдЯреА', ',', 'рд╕рдВрдЧрдарди', 'рдФрд░', 'рдЙрдирдХреА', 'рд╕рд░рдХрд╛рд░', 'рдкрд░', 'рднреА', 'рдкрдбрд╝рд╛', 'рдерд╛ред', 'рдЙрд╕', 'рд╕рдордп', 'рд░рдгрдЬреАрдд', 'рдХреА', 'рдирд╛рд░рд╛рдЬрдЧреА', 'рдХреЗ', 'рдЪрд▓рддреЗ', 'рдЙрдирдХреЗ', 'рд╕рдорд░реНрдерди', 'рдореЗрдВ', 'рдХрдИ', 'рдХреИрдмрд┐рдиреЗрдЯ', 'рдордВрддреНрд░рд┐рдпреЛрдВ', 'рдиреЗ', 'рдЗрд╕реНрддреАрдлреЗ', 'рджреЗ', 'рджрд┐рдП', 'рдереЗ', 'рдХрд┐рдиреНрддреБ', 'рддрдм', 'рдкрд╛рд░реНрдЯреА', 'рд╕реБрдкреНрд░реАрдореЛ', 'рдЪреМ', '.', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдХреА', 'рд╣рд░рд┐рдпрд╛рдгрд╛', 'рдХреА', 'рдЬрдирддрд╛', 'рдкрд░', 'рдЗрддрдиреА', 'рдордЬрдмреВрдд', 'рдкрдХрдбрд╝', 'рдереА', 'рдХрд┐', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдХреЛ', 'рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА', 'рдмрдирд╛рдиреЗ', 'рдХреЗ', 'рдЙрдирдХреЗ', 'рдлреИрд╕рд▓реЗ', 'рдХрд╛', 'рдЬрдирддрд╛', 'рдХреЗ', 'рдмреАрдЪ', 'рдХреЛрдИ', 'рдЦрд╛рд╕', 'рд╡рд┐рд░реЛрдз', 'рдирд╣реАрдВ', 'рд╣реБрдЖ', 'рдерд╛', 'рд▓реЗрдХрд┐рди', 'рдЖрдЬ', 'рд╕реНрдерд┐рддрд┐', 'рдмрд┐рд▓реНрдХреБрд▓', 'рд╡рд┐рдкрд░реАрдд', 'рд╣реИред', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдкрд┐рдЫрд▓реЗ', 'рдХрд╛рдлреА', 'рд╕рдордп', 'рд╕реЗ', 'рдЬреЗрд▓', 'рдореЗрдВ', 'рд╣реИрдВ', 'рдФрд░', 'рдЬреЗрд▓', 'рдореЗрдВ', 'рд░рд╣рддреЗ', 'рдкрд╛рд░реНрдЯреА', 'рдХреЗ', 'рд╕рд╛рде', '-', 'рд╕рд╛рде', 'рдкрд░рд┐рд╡рд╛рд░', 'рдкрд░', 'рднреА', 'рдЙрдирдХреА', 'рдкрдХрдбрд╝', 'рдХрд╛рдлреА', 'рдврд╝реАрд▓реА', 'рд╣реЛ', 'рдЧрдИ', 'рд╣реИ', ',', 'рдЗрд╕реА', 'рдХрд╛рд░рдг', 'рдЙрдирдореЗрдВ', 'рдЕрдм', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдЬреИрд╕рд╛', 'рд╡реЛ', 'рд╕рд╛рдорд░реНрдереНрдп', 'рдирдЬрд░', 'рдирд╣реАрдВ', 'рдЖрддрд╛', 'рдХрд┐', 'рд╡реЗ', 'рдЕрдкрдиреЗ', 'рдлреИрд╕рд▓реЛрдВ', 'рдХреЛ', 'рдмрдЧреИрд░', 'рдХрд┐рд╕реА', 'рдкреНрд░рддрд┐рд░реЛрдз', 'рдХреЗ', 'рд▓рд╛рдЧреВ', 'рдХрд░рд╛', 'рд╕рдХреЗрдВред', 'рдЬрд╣рд╛рдВ', 'рдЖрдИ', 'рдереА', 'рддрдмрд╛рд╣реА', 'рдЙрд╕', 'рдШрд╛рдЯреА', 'рдХреНрд╖реЗрддреНрд░', 'рдореЗрдВ', 'рдЦрддрд░рд╛', 'рдЬреНрдпрд╛рджрд╛']\n",
      "Number of Tokens: 258\n",
      "Total Characters: 1039\n",
      "Average Word Length: 4.11\n",
      "Type-Token Ratio: 0.597\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(sample['text'] for sample in samples)\n",
    "stats = hindi_corpus_statistics(text)\n",
    "\n",
    "print(\"Original Sentences:\")\n",
    "for sentence in stats['sentences']:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nReconstructed Sentences:\")\n",
    "for sent in stats['reconstructed_sentences']:\n",
    "    print(sent)\n",
    "\n",
    "print(\"\\nTokens:\", stats['tokens'])\n",
    "print(\"Number of Tokens:\", stats['num_tokens'])\n",
    "print(\"Total Characters:\", stats['total_characters'])\n",
    "print(\"Average Word Length:\", stats['average_word_length'])\n",
    "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e5b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def english_tokenizer(text):\n",
    "    # Define patterns\n",
    "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
    "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b'\n",
    "\n",
    "    # Save and replace URLs/emails\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    text = re.sub(url_pattern, '<URL>', text)\n",
    "\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token == '<URL>':\n",
    "            tokens.append(urls.pop(0))\n",
    "        elif token == '<EMAIL>':\n",
    "            tokens.append(emails.pop(0))\n",
    "        else:\n",
    "            # Tokenize: words, numbers, punctuation\n",
    "            split_tokens = re.findall(r\"[a-zA-Z0-9]+|[.,!?;:'\\\"()\\-]|[^\\s]\", token)\n",
    "            tokens.extend(split_tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def english_sentence_tokenizer(text):\n",
    "    # Split on sentence-ending punctuation followed by space\n",
    "    sentence_end_pattern = r'(?<=[.!?])\\s+'\n",
    "    sentences = re.split(sentence_end_pattern, text.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def detokenize(tokens):\n",
    "    sentence = ''\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i > 0 and not re.match(r'[.,!?;:)\\]\\'\\\"]', token):\n",
    "            sentence += ' '\n",
    "        sentence += token\n",
    "    return sentence.strip()\n",
    "\n",
    "def english_corpus_statistics(text):\n",
    "    sentences = english_sentence_tokenizer(text)\n",
    "    all_tokens = []\n",
    "    reconstructed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = english_tokenizer(sentence)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "        reconstructed = detokenize(tokens)\n",
    "        reconstructed_sentences.append(reconstructed)\n",
    "\n",
    "    num_tokens = len(all_tokens)\n",
    "    unique_tokens = set(all_tokens)\n",
    "    total_chars = sum(len(token) for token in all_tokens)\n",
    "\n",
    "    word_tokens = [t for t in all_tokens if re.match(r'^[a-zA-Z0-9]+$', t)]\n",
    "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
    "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
    "\n",
    "    return {\n",
    "        'sentences': sentences,\n",
    "        'tokens': all_tokens,\n",
    "        'num_tokens': num_tokens,\n",
    "        'total_characters': total_chars,\n",
    "        'average_word_length': round(avg_word_length, 2),\n",
    "        'type_token_ratio': round(type_token_ratio, 3),\n",
    "        'reconstructed_sentences': reconstructed_sentences\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe7ae645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "Hi!\n",
      "My name is John.\n",
      "Email me at john.doe@example.com or visit https://openai.com.\n",
      "This is amazing, isn't it?\n",
      "Let's test URLs like www.testsite.com too.\n",
      "\n",
      "Reconstructed Sentences:\n",
      "Hi!\n",
      "My name is John.\n",
      "Email me at john.doe@example.com or visit https://openai.com.\n",
      "This is amazing, isn' t it?\n",
      "Let' s test URLs like www.testsite.com too.\n",
      "\n",
      "Tokens: ['Hi', '!', 'My', 'name', 'is', 'John', '.', 'Email', 'me', 'at', 'john.doe@example.com', 'or', 'visit', 'https://openai.com.', 'This', 'is', 'amazing', ',', 'isn', \"'\", 't', 'it', '?', 'Let', \"'\", 's', 'test', 'URLs', 'like', 'www.testsite.com', 'too', '.']\n",
      "Number of Tokens: 32\n",
      "Total Characters: 130\n",
      "Average Word Length: 3.09\n",
      "Type-Token Ratio: 0.906\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hi! My name is John. Email me at john.doe@example.com or visit https://openai.com. \n",
    "This is amazing, isn't it? Let's test URLs like www.testsite.com too.\"\"\"\n",
    "\n",
    "stats = english_corpus_statistics(text)\n",
    "\n",
    "print(\"Original Sentences:\")\n",
    "for s in stats['sentences']:\n",
    "    print(s)\n",
    "\n",
    "print(\"\\nReconstructed Sentences:\")\n",
    "for s in stats['reconstructed_sentences']:\n",
    "    print(s)\n",
    "\n",
    "print(\"\\nTokens:\", stats['tokens'])\n",
    "print(\"Number of Tokens:\", stats['num_tokens'])\n",
    "print(\"Total Characters:\", stats['total_characters'])\n",
    "print(\"Average Word Length:\", stats['average_word_length'])\n",
    "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
