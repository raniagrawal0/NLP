{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4TiIWNUcLjB",
    "outputId": "2500606a-5239-4a79-d262-8616add14335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing ---\n",
      "Original: I scored 95 marks in the exam!\n",
      "Tokens: ['i', 'scored', 'NUMBER', 'marks', 'in', 'the', 'exam', 'PUNCT']\n",
      "\n",
      "Original: Visit https://example.com for more details.\n",
      "Tokens: ['visit', 'URL', 'for', 'more', 'details', 'PUNCT']\n",
      "\n",
      "Original: Numbers like 1000 or 3.14 are replaced.\n",
      "Tokens: ['numbers', 'like', 'NUMBER', 'or', 'NUMBER', 'are', 'replaced', 'PUNCT']\n",
      "\n",
      "Original: Punctuation, such as commas, should be replaced too!\n",
      "Tokens: ['punctuation', 'PUNCT', 'such', 'as', 'commas', 'PUNCT', 'should', 'be', 'replaced', 'too', 'PUNCT']\n",
      "\n",
      "\n",
      "--- Vocabulary ---\n",
      "['NUMBER', 'PUNCT', 'URL', 'are', 'as', 'be', 'commas', 'details', 'exam', 'for', 'i', 'in', 'like', 'marks', 'more', 'numbers', 'or', 'punctuation', 'replaced', 'scored', 'should', 'such', 'the', 'too', 'visit']\n",
      "\n",
      "--- TF-IDF Scores ---\n",
      "\n",
      "Sentence 1:\n",
      "NUMBER         : 0.1140\n",
      "URL            : 0.1079\n",
      "are            : 0.1079\n",
      "as             : 0.1079\n",
      "be             : 0.1079\n",
      "commas         : 0.1079\n",
      "details        : 0.1079\n",
      "exam           : 0.2045\n",
      "for            : 0.1079\n",
      "i              : 0.2045\n",
      "in             : 0.2045\n",
      "like           : 0.1079\n",
      "marks          : 0.2045\n",
      "more           : 0.1079\n",
      "numbers        : 0.1079\n",
      "or             : 0.1079\n",
      "punctuation    : 0.1079\n",
      "replaced       : 0.0602\n",
      "scored         : 0.2045\n",
      "should         : 0.1079\n",
      "such           : 0.1079\n",
      "the            : 0.2045\n",
      "too            : 0.1079\n",
      "visit          : 0.1079\n",
      "\n",
      "Sentence 2:\n",
      "NUMBER         : 0.0787\n",
      "URL            : 0.2636\n",
      "are            : 0.1412\n",
      "as             : 0.1412\n",
      "be             : 0.1412\n",
      "commas         : 0.1412\n",
      "details        : 0.2636\n",
      "exam           : 0.1412\n",
      "for            : 0.2636\n",
      "i              : 0.1412\n",
      "in             : 0.1412\n",
      "like           : 0.1412\n",
      "marks          : 0.1412\n",
      "more           : 0.2636\n",
      "numbers        : 0.1412\n",
      "or             : 0.1412\n",
      "punctuation    : 0.1412\n",
      "replaced       : 0.0787\n",
      "scored         : 0.1412\n",
      "should         : 0.1412\n",
      "such           : 0.1412\n",
      "the            : 0.1412\n",
      "too            : 0.1412\n",
      "visit          : 0.2636\n",
      "\n",
      "Sentence 3:\n",
      "NUMBER         : 0.1627\n",
      "URL            : 0.1079\n",
      "are            : 0.2045\n",
      "as             : 0.1079\n",
      "be             : 0.1079\n",
      "commas         : 0.1079\n",
      "details        : 0.1079\n",
      "exam           : 0.1079\n",
      "for            : 0.1079\n",
      "i              : 0.1079\n",
      "in             : 0.1079\n",
      "like           : 0.2045\n",
      "marks          : 0.1079\n",
      "more           : 0.1079\n",
      "numbers        : 0.2045\n",
      "or             : 0.2045\n",
      "punctuation    : 0.1079\n",
      "replaced       : 0.1140\n",
      "scored         : 0.1079\n",
      "should         : 0.1079\n",
      "such           : 0.1079\n",
      "the            : 0.1079\n",
      "too            : 0.1079\n",
      "visit          : 0.1079\n",
      "\n",
      "Sentence 4:\n",
      "NUMBER         : 0.0444\n",
      "URL            : 0.0797\n",
      "are            : 0.0797\n",
      "as             : 0.1531\n",
      "be             : 0.1531\n",
      "commas         : 0.1531\n",
      "details        : 0.0797\n",
      "exam           : 0.0797\n",
      "for            : 0.0797\n",
      "i              : 0.0797\n",
      "in             : 0.0797\n",
      "like           : 0.0797\n",
      "marks          : 0.0797\n",
      "more           : 0.0797\n",
      "numbers        : 0.0797\n",
      "or             : 0.0797\n",
      "punctuation    : 0.1531\n",
      "replaced       : 0.0853\n",
      "scored         : 0.0797\n",
      "should         : 0.1531\n",
      "such           : 0.1531\n",
      "the            : 0.0797\n",
      "too            : 0.1531\n",
      "visit          : 0.0797\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def preprocess(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Replace URLs\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', ' URL ', sentence)\n",
    "\n",
    "    # Replace numbers\n",
    "    sentence = re.sub(r'\\b\\d+(\\.\\d+)?\\b', ' NUMBER ', sentence)\n",
    "\n",
    "    # Replace punctuations\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' PUNCT ', sentence)\n",
    "\n",
    "    # Tokenize (split on whitespace)\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def compute_tf_with_normalization(sentence, vocab, smoothing=False):\n",
    "    tokens = preprocess(sentence)\n",
    "    total_terms = len(tokens)\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    tf = {}\n",
    "    for term in vocab:\n",
    "        if smoothing:\n",
    "            count = token_counts.get(term, 0) + 1\n",
    "        else:\n",
    "            count = token_counts.get(term, 0)\n",
    "        # Normalized TF\n",
    "        tf[term] = math.log(1 + count / total_terms)\n",
    "\n",
    "    return tf\n",
    "\n",
    "\n",
    "def compute_idf(sentences, vocab, smoothing=False):\n",
    "    N = len(sentences)\n",
    "    df = defaultdict(int)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = set(preprocess(sentence))\n",
    "        for term in vocab:\n",
    "            if term in tokens:\n",
    "                df[term] += 1\n",
    "\n",
    "    idf = {}\n",
    "    for term in vocab:\n",
    "        if smoothing:\n",
    "            idf[term] = math.log((N + 1) / (df[term] + 1))\n",
    "        else:\n",
    "            if df[term] > 0:\n",
    "                idf[term] = math.log(N / df[term])\n",
    "            else:\n",
    "                idf[term] = 0.0  # unseen word case\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_tf_idf_scores(sentences, smoothing=False):\n",
    "    # Build vocabulary\n",
    "    vocab = sorted(set(token for s in sentences for token in preprocess(s)))\n",
    "\n",
    "    idf = compute_idf(sentences, vocab, smoothing=smoothing)\n",
    "\n",
    "    tf_idf_all = []\n",
    "    for sentence in sentences:\n",
    "        tf = compute_tf_with_normalization(sentence, vocab, smoothing=smoothing)\n",
    "        tf_idf = {term: tf[term] * idf[term] for term in vocab}\n",
    "        tf_idf_all.append(tf_idf)\n",
    "\n",
    "    return vocab, tf_idf_all\n",
    "\n",
    "def main():\n",
    "    sentences = [\n",
    "        \"I scored 95 marks in the exam!\",\n",
    "        \"Visit https://example.com for more details.\",\n",
    "        \"Numbers like 1000 or 3.14 are replaced.\",\n",
    "        \"Punctuation, such as commas, should be replaced too!\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Preprocessing ---\")\n",
    "    for s in sentences:\n",
    "        print(f\"Original: {s}\")\n",
    "        print(f\"Tokens: {preprocess(s)}\\n\")\n",
    "\n",
    "    vocab, tfidf_scores = compute_tf_idf_scores(sentences, smoothing=True)\n",
    "\n",
    "    print(\"\\n--- Vocabulary ---\")\n",
    "    print(vocab)\n",
    "\n",
    "    print(\"\\n--- TF-IDF Scores ---\")\n",
    "    for i, sent_scores in enumerate(tfidf_scores):\n",
    "        print(f\"\\nSentence {i+1}:\")\n",
    "        for term, score in sent_scores.items():\n",
    "            if score > 0:\n",
    "                print(f\"{term:15s}: {score:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aCCF7KLacfhO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('e', '</w>')\n",
      "Merge 2: ('t', 'h')\n",
      "Merge 3: ('th', 'e</w>')\n",
      "Merge 4: ('s', '</w>')\n",
      "Merge 5: ('g', '</w>')\n",
      "Merge 6: ('d', 'o')\n",
      "Merge 7: ('b', 'o')\n",
      "Merge 8: ('bo', 'y')\n",
      "Merge 9: ('g', 's</w>')\n",
      "Merge 10: ('c', 'a')\n",
      "Merge 11: ('ca', 't')\n",
      "Merge 12: ('i', 'n')\n",
      "Merge 13: ('in', 'g</w>')\n",
      "Merge 14: ('boy', '</w>')\n",
      "Merge 15: ('h', 'u')\n",
      "Merge 16: ('cat', '</w>')\n",
      "Merge 17: ('a', 'r')\n",
      "Merge 18: ('ar', 'e</w>')\n",
      "Merge 19: ('do', 'gs</w>')\n",
      "Merge 20: ('do', 'g</w>')\n",
      "\n",
      "âœ… Normalized WordPiece Vocabulary:\n",
      "['', 'a', 'are', 'boy', 'c', 'cat', 'd', 'dog', 'dogs', 'e', 'g', 'gs', 'h', 'hu', 'i', 'ing', 'l', 'n', 'o', 'q', 's', 't', 'the', 'u', 'y']\n",
      "\n",
      "Tokenization of test sentence:\n",
      "['the', 'cat', 'i', '[UNK]', 'c', '[UNK]', 'the', 'dog', 'q', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Step 1: Dataset\n",
    "sentences = [\n",
    "    \"The boy hugs the cat.\",\n",
    "    \"The boys are hugging the dogs.\",\n",
    "    \"The dogs are chasing the cats.\",\n",
    "    \"The dog and the cat sit quietly.\",\n",
    "    \"The boy is sitting on the dog.\"\n",
    "]\n",
    "\n",
    "# Step 2: Preprocess (lowercase + remove punctuation)\n",
    "def preprocess(sent):\n",
    "    sent = re.sub(r'[^\\w\\s]', '', sent.lower())\n",
    "    return sent.split()\n",
    "\n",
    "corpus = [preprocess(s) for s in sentences]\n",
    "\n",
    "# Step 3: Convert each word into character tokens\n",
    "def get_initial_vocab(corpus):\n",
    "    word_freq = Counter()\n",
    "    for sent in corpus:\n",
    "        for word in sent:\n",
    "            chars = \" \".join(list(word)) + \" </w>\"  # Add end of word marker\n",
    "            word_freq[chars] += 1\n",
    "    return word_freq\n",
    "\n",
    "vocab = get_initial_vocab(corpus)\n",
    "\n",
    "# Step 4: Function to get pair frequencies\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "# Step 5: Merge the most frequent pair\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = pattern.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# Step 6: Apply 20 merge iterations\n",
    "num_merges = 20\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f\"Merge {i+1}: {best}\")\n",
    "\n",
    "# Step 7: Build final vocabulary\n",
    "normalized_vocab = set()\n",
    "for token in vocab:\n",
    "    for piece in token.split():\n",
    "        # remove </w> marker, add clean token\n",
    "        if piece.endswith(\"</w>\"):\n",
    "            normalized_vocab.add(piece.replace(\"</w>\", \"\"))\n",
    "        else:\n",
    "            normalized_vocab.add(piece)\n",
    "\n",
    "final_vocab = sorted(normalized_vocab)\n",
    "\n",
    "print(\"\\nâœ… Normalized WordPiece Vocabulary:\")\n",
    "print(final_vocab)\n",
    "\n",
    "\n",
    "# Step 8: Tokenize new sentence using learned vocab\n",
    "def wordpiece_tokenize(word, vocab):\n",
    "    if word in vocab:\n",
    "        return [word]\n",
    "    chars = list(word)\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(chars):\n",
    "        subtoken = None\n",
    "        for j in range(len(chars), i, -1):\n",
    "            piece = ''.join(chars[i:j])\n",
    "            if i > 0:\n",
    "                piece = '##' + piece\n",
    "            if piece in vocab:\n",
    "                subtoken = piece\n",
    "                break\n",
    "        if subtoken is None:\n",
    "            tokens.append('[UNK]')\n",
    "            break\n",
    "        tokens.append(subtoken)\n",
    "        i = j\n",
    "    return tokens\n",
    "\n",
    "# Step 9: Tokenize the new test sentence\n",
    "test_sentence = \"The cat is chasing the dog quietly.\"\n",
    "test_words = preprocess(test_sentence)\n",
    "\n",
    "print(\"\\nTokenization of test sentence:\")\n",
    "tokens = []\n",
    "for word in test_words:\n",
    "    tokens.extend(wordpiece_tokenize(word, final_vocab))\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessed Sentences:\n",
      "\n",
      "Inform : ['check', 'out', 'URL', 'for', 'more', 'info', 'PUNCT']\n",
      "Promo : ['order', 'NUMBER', 'items', 'PUNCT', 'get', 'NUMBER', 'free', 'PUNCT', 'limited', 'offer', 'PUNCT']\n",
      "Inform : ['your', 'package', 'PUNCT', 'NUMBER', 'will', 'arrive', 'tomorrow', 'PUNCT']\n",
      "Promo : ['win', '$NUMBER', 'now', 'PUNCT', 'visit', 'URL']\n",
      "Reminder : ['meeting', 'at', 'NUMBERpm', 'PUNCT', 'don', 'PUNCT', 't', 'forget', 'to', 'bring', 'the', 'files', 'PUNCT']\n",
      "Promo : ['exclusive', 'deal', 'for', 'you', 'PUNCT', 'buy', 'NUMBER', 'PUNCT', 'get', 'NUMBER', 'free', 'PUNCT']\n",
      "Inform : ['download', 'the', 'report', 'from', 'URL']\n",
      "Reminder : ['the', 'meeting', 'is', 'starting', 'in', 'NUMBER', 'minutes', 'PUNCT']\n",
      "Reminder : ['reminder', 'PUNCT', 'submit', 'your', 'timesheet', 'by', 'NUMBERpm', 'today', 'PUNCT']\n",
      "\n",
      "âœ… Class Priors: {'Inform': 0.3333333333333333, 'Promo': 0.3333333333333333, 'Reminder': 0.3333333333333333} \n",
      "\n",
      "âœ… Test Sentence (after preprocessing): ['you', 'will', 'get', 'an', 'exclusive', 'offer', 'in', 'the', 'meeting', 'PUNCT']\n",
      "\n",
      "Extracted Features: {'has_url': 0, 'has_number': 0, 'punct_count': 0, 'has_timeword': 1, 'bigrams': [('you', 'will'), ('will', 'get'), ('get', 'an'), ('an', 'exclusive'), ('exclusive', 'offer'), ('offer', 'in'), ('in', 'the'), ('the', 'meeting'), ('meeting', 'PUNCT')]}\n",
      "\n",
      "Class Scores:\n",
      "Inform    : -38.8548\n",
      "Promo     : -38.8055\n",
      "Reminder  : -36.7422\n",
      "\n",
      "ðŸŽ¯ Predicted Label: Reminder\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1. Training data\n",
    "# -------------------------------\n",
    "data = [\n",
    "    (\"Check out https://example.com for more info!\", \"Inform\"),\n",
    "    (\"Order 3 items, get 1 free! Limited offer!!!\", \"Promo\"),\n",
    "    (\"Your package #12345 will arrive tomorrow.\", \"Inform\"),\n",
    "    (\"Win $1000 now, visit http://winbig.com!!!\", \"Promo\"),\n",
    "    (\"Meeting at 3pm, don't forget to bring the files.\", \"Reminder\"),\n",
    "    (\"Exclusive deal for you: buy 2, get 1 free!!!\", \"Promo\"),\n",
    "    (\"Download the report from https://reports.com.\", \"Inform\"),\n",
    "    (\"The meeting is starting in 10 minutes.\", \"Reminder\"),\n",
    "    (\"Reminder: submit your timesheet by 5pm today.\", \"Reminder\"),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2. Preprocessing function\n",
    "# -------------------------------\n",
    "def preprocess(sentence):\n",
    "    # Lowercase\n",
    "    s = sentence.lower()\n",
    "\n",
    "    # Replace URLs\n",
    "    s = re.sub(r'http\\S+|www\\S+', 'URL', s)\n",
    "\n",
    "    # Replace numbers\n",
    "    s = re.sub(r'\\d+(\\.\\d+)?', 'NUMBER', s)\n",
    "\n",
    "    # Replace punctuation with token PUNCT (keep spaces around for token separation)\n",
    "    s = re.sub(r'[!.,?#:;\\'\"-]+', ' PUNCT ', s)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    return s.split()\n",
    "\n",
    "# Preprocess all sentences\n",
    "processed_data = [(preprocess(s), label) for s, label in data]\n",
    "\n",
    "print(\"âœ… Preprocessed Sentences:\\n\")\n",
    "for tokens, label in processed_data:\n",
    "    print(label, \":\", tokens)\n",
    "print()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3. Feature extraction functions\n",
    "# -------------------------------\n",
    "\n",
    "def extract_features(tokens):\n",
    "    features = {}\n",
    "\n",
    "    # Binary/frequency features\n",
    "    features[\"has_url\"] = int(\"url\" in tokens)\n",
    "    features[\"has_number\"] = int(\"number\" in tokens)\n",
    "    features[\"punct_count\"] = tokens.count(\"punct\")\n",
    "    features[\"has_timeword\"] = int(any(t in tokens for t in [\"pm\", \"am\", \"meeting\", \"minutes\", \"today\"]))\n",
    "\n",
    "    # Bigrams (for probability model)\n",
    "    bigrams = list(zip(tokens[:-1], tokens[1:])) if len(tokens) > 1 else []\n",
    "    features[\"bigrams\"] = bigrams\n",
    "    return features\n",
    "\n",
    "# Collect features per class\n",
    "class_features = defaultdict(list)\n",
    "for tokens, label in processed_data:\n",
    "    feats = extract_features(tokens)\n",
    "    class_features[label].append(feats)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4. Calculate priors\n",
    "# -------------------------------\n",
    "labels = [label for _, label in processed_data]\n",
    "label_counts = Counter(labels)\n",
    "total_docs = len(labels)\n",
    "priors = {lbl: label_counts[lbl] / total_docs for lbl in label_counts}\n",
    "\n",
    "print(\"âœ… Class Priors:\", priors, \"\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5. Build bigram probabilities (Add-K smoothing)\n",
    "# -------------------------------\n",
    "K = 0.3\n",
    "class_bigram_counts = defaultdict(Counter)\n",
    "class_unigram_counts = defaultdict(Counter)\n",
    "vocab_bigrams = set()\n",
    "\n",
    "for lbl, feats_list in class_features.items():\n",
    "    for feats in feats_list:\n",
    "        for (w1, w2) in feats[\"bigrams\"]:\n",
    "            class_bigram_counts[lbl][(w1, w2)] += 1\n",
    "            class_unigram_counts[lbl][w1] += 1\n",
    "            vocab_bigrams.add((w1, w2))\n",
    "\n",
    "V = len(vocab_bigrams)\n",
    "\n",
    "# Compute conditional probabilities P(w2|w1, class)\n",
    "bigram_probs = defaultdict(dict)\n",
    "for lbl in class_bigram_counts:\n",
    "    for (w1, w2) in vocab_bigrams:\n",
    "        count_bigram = class_bigram_counts[lbl][(w1, w2)]\n",
    "        count_w1 = class_unigram_counts[lbl][w1]\n",
    "        prob = (count_bigram + K) / (count_w1 + K * V)\n",
    "        bigram_probs[lbl][(w1, w2)] = prob\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6. Naive Bayes Prediction\n",
    "# -------------------------------\n",
    "def predict(sentence):\n",
    "    tokens = preprocess(sentence)\n",
    "    feats = extract_features(tokens)\n",
    "\n",
    "    scores = {}\n",
    "    for lbl in priors:\n",
    "        # Start with log prior\n",
    "        log_prob = math.log(priors[lbl])\n",
    "\n",
    "        # Binary/frequency features (simple log multipliers)\n",
    "        if feats[\"has_url\"]:\n",
    "            log_prob += math.log(1.5) if lbl == \"Inform\" else math.log(0.8)\n",
    "        if feats[\"punct_count\"] >= 2 and lbl == \"Promo\":\n",
    "            log_prob += math.log(2.0)\n",
    "        if feats[\"has_timeword\"] and lbl == \"Reminder\":\n",
    "            log_prob += math.log(2.0)\n",
    "\n",
    "        # Bigram probabilities\n",
    "        for (w1, w2) in feats[\"bigrams\"]:\n",
    "            if (w1, w2) in bigram_probs[lbl]:\n",
    "                log_prob += math.log(bigram_probs[lbl][(w1, w2)])\n",
    "            else:\n",
    "                # unseen bigram\n",
    "                log_prob += math.log(K / (K * V))\n",
    "\n",
    "        scores[lbl] = log_prob\n",
    "\n",
    "    predicted = max(scores, key=scores.get)\n",
    "    return predicted, scores, feats\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7. Test on given sentence\n",
    "# -------------------------------\n",
    "test_sentence = \"You will get an exclusive offer in the meeting!\"\n",
    "pred_label, scores, feats = predict(test_sentence)\n",
    "\n",
    "print(\"âœ… Test Sentence (after preprocessing):\", preprocess(test_sentence))\n",
    "print(\"\\nExtracted Features:\", feats)\n",
    "print(\"\\nClass Scores:\")\n",
    "for lbl, sc in scores.items():\n",
    "    print(f\"{lbl:10s}: {sc:.4f}\")\n",
    "print(\"\\nðŸŽ¯ Predicted Label:\", pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
