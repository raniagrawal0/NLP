{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d71ad61-a7c4-4788-a5b9-b10547c75a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 sentences.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os, json, pickle, random,re\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT = \"C://Users/rani/Desktop/nlp lab/lab1/hindi_tokens.txt\"   # will try .json .pkl .txt or bare file\n",
    "VAL_SIZE = 1000\n",
    "TEST_SIZE = 1000\n",
    "SEED = 42\n",
    "max_sentences = 10000 \n",
    "def load_sentences(base):\n",
    "    candidates = [base, base + \".json\", base + \".pkl\", base + \".txt\"]\n",
    "    for f in candidates:\n",
    "        if not os.path.exists(f):\n",
    "            continue\n",
    "        sentences = []\n",
    "        sentence_count = 0\n",
    "        if f.endswith(\".txt\"):\n",
    "            with open(f, \"r\", encoding=\"utf8\") as fh:\n",
    "                for line in fh:\n",
    "                    # Split paragraph into sentences by punctuation marks\n",
    "                    parts = re.split(r'[।!?.]', line)\n",
    "                    for sentence in parts:\n",
    "                        sentence = sentence.strip()\n",
    "                        if sentence:\n",
    "                            tokens = sentence.split()\n",
    "                            if tokens:\n",
    "                                sentences.append(tokens)\n",
    "                                sentence_count += 1\n",
    "                                if sentence_count >= max_sentences:\n",
    "                                    break\n",
    "                    if sentence_count >= max_sentences:\n",
    "                        break\n",
    "            return sentences\n",
    "        \n",
    "\n",
    "sentences = load_sentences(INPUT)\n",
    "print(f\"Loaded {len(sentences)} sentences.\")\n",
    "random.seed(SEED)\n",
    "random.shuffle(sentences)\n",
    "if len(sentences) < VAL_SIZE + TEST_SIZE:\n",
    "    raise ValueError(\"Not enough sentences to create the requested splits.\")\n",
    "val = sentences[:VAL_SIZE]\n",
    "test = sentences[VAL_SIZE:VAL_SIZE+TEST_SIZE]\n",
    "train = sentences[VAL_SIZE+TEST_SIZE:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a70196-4b09-4b37-9d68-db5549d1c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_pmi(unigram_counts, bigram_counts, total_unigrams):\n",
    "    \"\"\"Compute PMI for all bigrams seen.\"\"\"\n",
    "    pmi = {}\n",
    "    for (w1, w2), c12 in bigram_counts.items():\n",
    "        p_w1 = unigram_counts.get((w1,), 0) / total_unigrams\n",
    "        p_w2 = unigram_counts.get((w2,), 0) / total_unigrams\n",
    "        p_w1w2 = c12 / (total_unigrams - 1)\n",
    "        if p_w1 > 0 and p_w2 > 0 and p_w1w2 > 0:\n",
    "            pmi[(w1, w2)] = math.log2(p_w1w2 / (p_w1 * p_w2))\n",
    "    return pmi\n",
    "\n",
    "def score_bigrams_in_set(tokenized_sentences, pmi_dict):\n",
    "    scores = []\n",
    "    for sent in tokenized_sentences:\n",
    "        for i in range(len(sent)-1):\n",
    "            bigram = (sent[i], sent[i+1])\n",
    "            if bigram in pmi_dict:\n",
    "                scores.append((bigram, pmi_dict[bigram]))\n",
    "    return scores\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert your tokenized sentences to text strings\n",
    "def detokenize(sentences):\n",
    "    return [\" \".join(tokens) for tokens in sentences]\n",
    "\n",
    "train_texts = detokenize(train)\n",
    "val_texts = detokenize(val)\n",
    "test_texts = detokenize(test)\n",
    "\n",
    "# Learn IDF on train only\n",
    "tfidf = TfidfVectorizer(min_df=2, ngram_range=(1,2))  # you can include bigrams\n",
    "X_train = tfidf.fit_transform(train_texts)\n",
    "X_val = tfidf.transform(val_texts)\n",
    "X_test = tfidf.transform(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6da41d0-f228-4ece-a541-fe8376d3cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9680\n",
      "Sample features: ['00' '00 लर' '000' '000 तक' '000 पय' '01' '02' '04' '04 तत' '042'\n",
      " '042 पय' '05' '06' '07' '08' '08 58' '09' '10' '10 000' '10 30']\n"
     ]
    }
   ],
   "source": [
    "features = tfidf.get_feature_names_out()\n",
    "print(\"Vocabulary size:\", len(features))\n",
    "print(\"Sample features:\", features[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "750398d0-3272-4af4-bfe5-cbe47a3bdc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     00  00 लर       000  000 तक  000 पय   01   02   04  04 तत  042  ...  \\\n",
      "20  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "11  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "42  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "35  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "28  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "13  0.0    0.0  0.437387     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "19  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "33  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "38  0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "1   0.0    0.0  0.000000     0.0     0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "\n",
      "    हस षर  हसन  हड़क   ज़त  ज़त बर   ड़क  ड़कर   ड़छ   ड़त   ड़न  \n",
      "20    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "42    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "35    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "28    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "13    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "19    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "33    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "38    0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1     0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[10 rows x 9680 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample = X_train[:50].toarray()  # first 5 sentences\n",
    "df = pd.DataFrame(sample, columns=features)\n",
    "print(df.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c3132c3-0ea3-4f8e-8b72-596560ef4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TF–IDF words for sentence 0 :\n",
      "धन -> 0.7540\n",
      "आर -> 0.6569\n"
     ]
    }
   ],
   "source": [
    "def top_words_for_sentence(X, features, index, top_n=10):\n",
    "    row = X[index].toarray()[0]\n",
    "    top_idx = row.argsort()[-top_n:][::-1]\n",
    "    print(\"Top TF–IDF words for sentence\", index, \":\")\n",
    "    for i in top_idx:\n",
    "        if row[i] > 0:\n",
    "            print(f\"{features[i]} -> {row[i]:.4f}\")\n",
    "\n",
    "top_words_for_sentence(X_train, features, 0, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5c79afe-6ce9-45a6-85d3-13f685af9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = tfidf.get_feature_names_out()\n",
    "with open(\"tfidf_readable.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(X_train.shape[0]):  # each sentence\n",
    "        row = X_train[i].toarray()[0]\n",
    "        nz_idx = row.nonzero()[0]\n",
    "        words_weights = [(features[j], row[j]) for j in nz_idx]\n",
    "        words_weights = sorted(words_weights, key=lambda x: x[1], reverse=True)[:10]  # top 10\n",
    "        f.write(f\"Sentence {i}:\\n\")\n",
    "        for word, weight in words_weights:\n",
    "            f.write(f\"  {word}: {weight:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce7ac463-c632-49b2-8f75-8c3def94ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def nearest_neighbors_within_set(X):\n",
    "    # compute cosine similarity matrix\n",
    "    sim_matrix = cosine_similarity(X)\n",
    "    np.fill_diagonal(sim_matrix, -1)  # ignore self-similarity\n",
    "    nearest_idx = np.argmax(sim_matrix, axis=1)\n",
    "    return nearest_idx, np.max(sim_matrix, axis=1)\n",
    "\n",
    "val_nn_idx, val_nn_score = nearest_neighbors_within_set(X_val)\n",
    "test_nn_idx, test_nn_score = nearest_neighbors_within_set(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0711fcb5-3a0b-424c-b5e7-c30524091a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sentence: जरुरत की चीजें गांव की दुकानों से खरीदते हैं\n",
      "Nearest neighbor: व्यापारिक समूह को इससे चिन्तित होने की जरूरत नहीं\n",
      "Similarity score: 0.4398336019931508\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation sentence:\", val_texts[0])\n",
    "print(\"Nearest neighbor:\", val_texts[val_nn_idx[0]])\n",
    "print(\"Similarity score:\", val_nn_score[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583a9e0-5829-42c9-9943-4805e2dc7f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
