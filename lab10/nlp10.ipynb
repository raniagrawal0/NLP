{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e02ca53-9680-4e90-823b-50da1528d22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences Loaded: 3914\n",
      "\n",
      "--- Starting Fold 1/5 ---\n",
      "Training Complete. Found 45 unique tags.\n",
      "Token-level Accuracy (F1-Score): 85.65%\n",
      "  Precision: 0.8565, Recall: 0.8565, F1-Score: 0.8565\n",
      "\n",
      "--- Starting Fold 2/5 ---\n",
      "Training Complete. Found 45 unique tags.\n",
      "Token-level Accuracy (F1-Score): 85.09%\n",
      "  Precision: 0.8509, Recall: 0.8509, F1-Score: 0.8509\n",
      "\n",
      "--- Starting Fold 3/5 ---\n",
      "Training Complete. Found 44 unique tags.\n",
      "Token-level Accuracy (F1-Score): 85.97%\n",
      "  Precision: 0.8597, Recall: 0.8597, F1-Score: 0.8597\n",
      "\n",
      "--- Starting Fold 4/5 ---\n",
      "Training Complete. Found 45 unique tags.\n",
      "Token-level Accuracy (F1-Score): 84.95%\n",
      "  Precision: 0.8495, Recall: 0.8495, F1-Score: 0.8495\n",
      "\n",
      "--- Starting Fold 5/5 ---\n",
      "Training Complete. Found 45 unique tags.\n",
      "Token-level Accuracy (F1-Score): 84.97%\n",
      "  Precision: 0.8497, Recall: 0.8497, F1-Score: 0.8497\n",
      "\n",
      "====================================\n",
      "**Final 5-Fold Cross-Validation Results**\n",
      "====================================\n",
      "Average Precision: 0.8533\n",
      "Average Recall:    0.8533\n",
      "Average F1-Score:  0.8533 (Overall Tagger Accuracy)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# --- Utility Functions ---\n",
    "# --- Utility Functions ---\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads and tokenizes the POS-tagged sentences in word_TAG format.\"\"\"\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                sentence = []\n",
    "                for word_tag in line.split():\n",
    "                    if '_' not in word_tag:\n",
    "                        continue  # skip malformed items\n",
    "                    # Use rsplit('_',1) to handle words with underscores\n",
    "                    word, tag = word_tag.rsplit('_', 1)\n",
    "                    sentence.append((word, tag))\n",
    "                sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def train_hmm(train_data):\n",
    "\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "    vocab = set()\n",
    "\n",
    "    all_tags = set()\n",
    "\n",
    "    # Count occurrences\n",
    "    for sentence in train_data:\n",
    "        prev_tag = \"<START>\"\n",
    "        tag_counts[\"<START>\"] += 1\n",
    "\n",
    "        for word, tag in sentence:\n",
    "            vocab.add(word)\n",
    "            all_tags.add(tag)\n",
    "\n",
    "            emission_counts[tag][word] += 1\n",
    "            tag_counts[tag] += 1\n",
    "\n",
    "            transition_counts[prev_tag][tag] += 1\n",
    "            prev_tag = tag\n",
    "\n",
    "        transition_counts[prev_tag][\"<END>\"] += 1\n",
    "\n",
    "    all_tags = list(all_tags)\n",
    "\n",
    "    # ------------------------\n",
    "    # Laplace smoothing\n",
    "    # ------------------------\n",
    "    V = len(vocab)\n",
    "    K = len(all_tags)\n",
    "\n",
    "    # Transition probabilities\n",
    "    T = defaultdict(dict)\n",
    "    for prev_tag in transition_counts:\n",
    "        total = sum(transition_counts[prev_tag].values())\n",
    "        for tag in all_tags + [\"<END>\"]:\n",
    "            T[prev_tag][tag] = (transition_counts[prev_tag].get(tag, 0) + 1) / (total + K + 1)\n",
    "\n",
    "    # START transitions: ensure smoothing\n",
    "    if \"<START>\" not in T:\n",
    "        T[\"<START>\"] = {}\n",
    "    for tag in all_tags:\n",
    "        T[\"<START>\"][tag] = (transition_counts[\"<START>\"].get(tag, 0) + 1) / (tag_counts[\"<START>\"] + K)\n",
    "\n",
    "    # Emission probabilities\n",
    "    E = defaultdict(dict)\n",
    "    for tag in all_tags:\n",
    "        total = sum(emission_counts[tag].values())\n",
    "        for word in vocab:\n",
    "            E[tag][word] = (emission_counts[tag].get(word, 0) + 1) / (total + V + 1)\n",
    "\n",
    "    return T, E, all_tags, vocab\n",
    "\n",
    "\n",
    "def viterbi_decode(sentence, T, E, all_tags, vocab, unk_prob=1e-6):\n",
    "\n",
    "    N = len(sentence)\n",
    "    if N == 0:\n",
    "        return []\n",
    "\n",
    "    # Viterbi probability table\n",
    "    V = [{} for _ in range(N)]\n",
    "    # Backpointer table\n",
    "    BP = [{} for _ in range(N)]\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Initialization\n",
    "    # ---------------------------\n",
    "    for tag in all_tags:\n",
    "\n",
    "        # Emission probability with smoothing\n",
    "        if sentence[0] in E[tag]:\n",
    "            emit = E[tag][sentence[0]]\n",
    "        else:\n",
    "            emit = unk_prob\n",
    "        \n",
    "        # Transition from START\n",
    "        trans = T[\"<START>\"].get(tag, 0)\n",
    "\n",
    "        V[0][tag] = trans * emit\n",
    "        BP[0][tag] = None\n",
    "\n",
    "    # If all are 0, fallback\n",
    "    if sum(V[0].values()) == 0:\n",
    "        return [\"NN\"] * N\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Viterbi DP\n",
    "    # ---------------------------\n",
    "    for i in range(1, N):\n",
    "        word = sentence[i]\n",
    "\n",
    "        for tag in all_tags:\n",
    "\n",
    "            # Emission with smoothing\n",
    "            if word in E[tag]:\n",
    "                emit = E[tag][word]\n",
    "            else:\n",
    "                emit = unk_prob\n",
    "\n",
    "            max_prob = -1\n",
    "            best_prev = None\n",
    "\n",
    "            for prev_tag in all_tags:\n",
    "\n",
    "                trans = T[prev_tag].get(tag, 0)\n",
    "                prob = V[i-1][prev_tag] * trans * emit\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_prev = prev_tag\n",
    "            \n",
    "            V[i][tag] = max_prob\n",
    "            BP[i][tag] = best_prev\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Backtrack best path\n",
    "    # ---------------------------\n",
    "    final_tag = max(V[N-1], key=V[N-1].get)\n",
    "\n",
    "    tags = [final_tag]\n",
    "    for i in range(N-1, 0, -1):\n",
    "        tags.append(BP[i][tags[-1]])\n",
    "\n",
    "    return list(reversed(tags))\n",
    "\n",
    "\n",
    "# --- Evaluation Functions ---\n",
    "\n",
    "def evaluate_tagger(true_tags, predicted_tags):\n",
    "    \"\"\"Calculates Precision, Recall, and F1-score.\"\"\"\n",
    "    \n",
    "    # We use micro-averaging for a single overall score (simpler for exam)\n",
    "    \n",
    "    # Ensure all lists are flat and of the same length\n",
    "    y_true = [tag for sublist in true_tags for tag in sublist]\n",
    "    y_pred = [tag for sublist in predicted_tags for tag in sublist]\n",
    "    \n",
    "    if not y_true or len(y_true) != len(y_pred):\n",
    "        return 0, 0, 0 # Avoid division by zero\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    correct_predictions = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    precision = accuracy\n",
    "    recall = accuracy\n",
    "    f1_score = accuracy\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "\n",
    "# a. Load data\n",
    "file_path = 'wsj_pos_tagged_en.txt'\n",
    "data = load_data(file_path)\n",
    "print(f\"Total Sentences Loaded: {len(data)}\")\n",
    "\n",
    "# a. Split the data into K folds (K >= 3)\n",
    "K = 5 \n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "    print(f\"\\n--- Starting Fold {fold + 1}/{K} ---\")\n",
    "\n",
    "    # Split data\n",
    "    train_data = [data[i] for i in train_index]\n",
    "    test_data = [data[i] for i in test_index]\n",
    "\n",
    "    # b. Find the emission and transition probabilities from the training data.\n",
    "    T, E, all_tags, vocab = train_hmm(train_data)\n",
    "    print(f\"Training Complete. Found {len(all_tags)} unique tags.\")\n",
    "\n",
    "    # c. Implement the Viterbi decoding and d. Evaluate the performance\n",
    "    true_tags = []\n",
    "    predicted_tags = []\n",
    "    \n",
    "    for sentence_data in test_data:\n",
    "        # Extract the sequence of words for decoding\n",
    "        words = [word for word, tag in sentence_data]\n",
    "        # Extract the true tag sequence for evaluation\n",
    "        true_tag_sequence = [tag for word, tag in sentence_data]\n",
    "        \n",
    "        # Predict the tag sequence\n",
    "        predicted_tag_sequence = viterbi_decode(words, T, E, all_tags,vocab)\n",
    "        \n",
    "        true_tags.append(true_tag_sequence)\n",
    "        predicted_tags.append(predicted_tag_sequence)\n",
    "\n",
    "    # d. Evaluate the performance\n",
    "    P, R, F1 = evaluate_tagger(true_tags, predicted_tags)\n",
    "    all_metrics.append((P, R, F1))\n",
    "\n",
    "    print(f\"Token-level Accuracy (F1-Score): {F1 * 100:.2f}%\")\n",
    "    print(f\"  Precision: {P:.4f}, Recall: {R:.4f}, F1-Score: {F1:.4f}\")\n",
    "\n",
    "# d. Final Summary across all folds\n",
    "avg_P = np.mean([m[0] for m in all_metrics])\n",
    "avg_R = np.mean([m[1] for m in all_metrics])\n",
    "avg_F1 = np.mean([m[2] for m in all_metrics])\n",
    "\n",
    "print(\"\\n====================================\")\n",
    "print(f\"**Final {K}-Fold Cross-Validation Results**\")\n",
    "print(\"====================================\")\n",
    "print(f\"Average Precision: {avg_P:.4f}\")\n",
    "print(f\"Average Recall:    {avg_R:.4f}\")\n",
    "print(f\"Average F1-Score:  {avg_F1:.4f} (Overall Tagger Accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85062730-d0d1-477b-9961-1b9c8baced28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
