{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31397baf-a9c5-4367-9b7e-c660778cb477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training Byte Pair Encoding (BPE)...\n",
      "[BPE] Starting training with 454 unique words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPE merges:   1%|‚ñè         | 451/32000 [00:00<00:17, 1842.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BPE] Done 451 merges.\n",
      "\n",
      "üöÄ Training WordPiece...\n",
      "[WordPiece] Initial subword vocab = 455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WordPiece merges: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31545/31545 [00:42<00:00, 742.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordPiece] Final vocab size ‚âà 456\n",
      "\n",
      "‚úÖ Training complete!\n",
      "üìÇ Files saved in: C:\\Users\\rani\\Desktop\\nlp lab\\lab9\\output1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: read tokenized corpus\n",
    "# ---------------------------------------------------------------------\n",
    "def read_corpus(path):\n",
    "    \"\"\"Each line = tokens separated by space.\"\"\"\n",
    "    corpus = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            toks = line.strip().split()\n",
    "            if toks:\n",
    "                corpus.append(toks)\n",
    "    return corpus\n",
    "def stream_corpus(filepath, limit=None):\n",
    "    corpus = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            corpus.append(line)\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "    return \" \".join(corpus)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Common: vocabulary counting\n",
    "# ---------------------------------------------------------------------\n",
    "def get_vocab(sentences):\n",
    "    vocab = Counter()\n",
    "    for tokens in sentences:\n",
    "        for tok in tokens:\n",
    "            vocab[tok] += 1\n",
    "    return vocab\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# --------------------------  BPE  ------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def bpe_get_stats(vocab):\n",
    "    \"\"\"Count frequency of symbol pairs across all words.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def bpe_merge_vocab(vocab, pair):\n",
    "    \"\"\"Merge given symbol pair in all words.\"\"\"\n",
    "    a, b = pair\n",
    "    pattern = ' '.join(pair)\n",
    "    replacement = a + b\n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(pattern, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    return new_vocab\n",
    "\n",
    "def train_bpe(tokenized_corpus, num_merges=32000):\n",
    "    # 1Ô∏è‚É£ Prepare initial vocab (char-level)\n",
    "    vocab = Counter()\n",
    "    for sent in tokenized_corpus:\n",
    "        for word in sent:\n",
    "            chars = ' '.join(list(word)) + ' </w>'\n",
    "            vocab[chars] += 1\n",
    "\n",
    "    merges = []\n",
    "    print(f\"[BPE] Starting training with {len(vocab)} unique words...\")\n",
    "    for i in tqdm(range(num_merges), desc=\"BPE merges\"):\n",
    "        pairs = bpe_get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = bpe_merge_vocab(vocab, best)\n",
    "        merges.append(best)\n",
    "    print(f\"[BPE] Done {len(merges)} merges.\")\n",
    "    return merges, vocab\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# -----------------------  WORDPIECE  ---------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def wordpiece_train(tokenized_corpus, target_vocab_size=32000):\n",
    "    word_freqs = Counter()\n",
    "    for sent in tokenized_corpus:\n",
    "        for word in sent:\n",
    "            word_freqs[word] += 1\n",
    "\n",
    "    # Initialize subwords as individual characters + '</w>'\n",
    "    subword_vocab = Counter()\n",
    "    for w, f in word_freqs.items():\n",
    "        for ch in list(w) + ['</w>']:\n",
    "            subword_vocab[ch] += f\n",
    "\n",
    "    merges = []\n",
    "    print(f\"[WordPiece] Initial subword vocab = {len(subword_vocab)}\")\n",
    "\n",
    "    for i in tqdm(range(target_vocab_size - len(subword_vocab)), desc=\"WordPiece merges\"):\n",
    "        # Count pair frequencies\n",
    "        pair_freqs = Counter()\n",
    "        for w, f in word_freqs.items():\n",
    "            symbols = list(w) + ['</w>']\n",
    "            for i2 in range(len(symbols) - 1):\n",
    "                pair_freqs[(symbols[i2], symbols[i2 + 1])] += f\n",
    "\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "\n",
    "        # Compute likelihood score = freq(pair) / (freq(a) * freq(b))\n",
    "        best_pair, best_score = None, -1.0\n",
    "        for (a, b), freq in pair_freqs.items():\n",
    "            score = freq / (subword_vocab[a] * subword_vocab[b] + 1e-10)\n",
    "            if score > best_score:\n",
    "                best_pair, best_score = (a, b), score\n",
    "\n",
    "        # Merge best pair\n",
    "        new_token = a + b\n",
    "        subword_vocab[new_token] = pair_freqs[best_pair]\n",
    "        merges.append(best_pair)\n",
    "        if len(subword_vocab) >= target_vocab_size:\n",
    "            break\n",
    "\n",
    "    print(f\"[WordPiece] Final vocab size ‚âà {len(subword_vocab)}\")\n",
    "    return merges, subword_vocab\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# --------------------------  MAIN  -----------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import os, json\n",
    "\n",
    "    input_file = r\"C:\\Users\\rani\\Desktop\\nlp lab\\lab1\\hindi_tokens.txt\"\n",
    "\n",
    "    outdir = r\"C:\\Users\\rani\\Desktop\\nlp lab\\lab9\\output1\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    corpus = stream_corpus(input_file, limit=100000)  # limit lines for testing\n",
    "\n",
    "    num_merges = 32000\n",
    "    vocab_size = 32000\n",
    "\n",
    "    # ------------------ üîπ BPE Algorithm ------------------\n",
    "    print(\"\\nüöÄ Training Byte Pair Encoding (BPE)...\")\n",
    "    merges_bpe, final_vocab_bpe = train_bpe(corpus, num_merges=num_merges)\n",
    "\n",
    "    with open(os.path.join(outdir, \"bpe_merges.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merges_bpe, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(os.path.join(outdir, \"bpe_vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(final_vocab_bpe.keys()), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # ------------------ üîπ WordPiece Algorithm ------------------\n",
    "    print(\"\\nüöÄ Training WordPiece...\")\n",
    "    merges_wp, vocab_wp = wordpiece_train(corpus, target_vocab_size=vocab_size)\n",
    "\n",
    "    with open(os.path.join(outdir, \"wordpiece_merges.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merges_wp, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(os.path.join(outdir, \"wordpiece_vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(vocab_wp.keys()), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # ‚úÖ Step 5: Confirm output\n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    print(\"üìÇ Files saved in:\", outdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c847b30-81ea-40f9-b919-e39be28b00a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
