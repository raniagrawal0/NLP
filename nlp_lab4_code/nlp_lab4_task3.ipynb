{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916cff58-5b84-4806-b3f2-da2935e4f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import re\n",
    "\n",
    "bigram_file = \"bigrams_add1.txt\"       \n",
    "unigram_file = \"unigrams_add1.txt\"       \n",
    "sentences_file = \"sentences.txt\"  \n",
    "out_csv = \"sentence_probabilities.csv\"\n",
    "max_sentences = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3312a3bb-cb47-4eb6-bb42-690245559cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LM: 510917 unigrams\n",
      "Loaded LM: 4604982 bigrams\n",
      "Processing 646 sentences\n",
      "Saved results to sentence_probabilities.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load LM\n",
    "unigram_probs = {}\n",
    "bigram_probs = {}\n",
    "with open(unigram_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            word, prob = parts\n",
    "            unigram_probs[word] = float(prob)\n",
    "        elif len(parts) == 3:\n",
    "            w1, w2, prob = parts\n",
    "            bigram_probs[(w1, w2)] = float(prob)\n",
    "print(f\"Loaded LM: {len(unigram_probs)} unigrams\")\n",
    "\n",
    "with open(bigram_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            word, prob = parts\n",
    "            unigram_probs[word] = float(prob)\n",
    "        elif len(parts) == 3:\n",
    "            w1, w2, prob = parts\n",
    "            bigram_probs[(w1, w2)] = float(prob)\n",
    "print(f\"Loaded LM: {len(bigram_probs)} bigrams\")\n",
    "\n",
    "# Load sentences\n",
    "with open(sentences_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    # Split sentences using Hindi danda (।) or punctuation\n",
    "    sentence = re.split(r'[।!?.]', text)\n",
    "    sentence = [s.strip() for s in sentence if s.strip()]\n",
    "    sentences.extend(sentence)\n",
    "\n",
    "sentences = sentences[:max_sentences]\n",
    "print(f\"Processing {len(sentences)} sentences\")\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r'[\\u0900-\\u097F]+', sentence.lower())\n",
    "\n",
    "def sentence_prob_unigram(tokens, unigram_probs):\n",
    "    log_prob = 0.0\n",
    "    for w in tokens + ['</s>']:\n",
    "        prob = unigram_probs.get(w, unigram_probs.get('<unk>', 1e-8))\n",
    "        log_prob += math.log(prob)\n",
    "    return log_prob\n",
    "\n",
    "def sentence_prob_bigram(tokens, bigram_probs, unigram_probs):\n",
    "    seq = ['<s>'] + tokens + ['</s>']\n",
    "    log_prob = 0.0\n",
    "    for i in range(1, len(seq)):\n",
    "        w1, w2 = seq[i-1], seq[i]\n",
    "        prob = bigram_probs.get((w1, w2), None)\n",
    "        if prob is None:\n",
    "            prob = unigram_probs.get(w2, unigram_probs.get('<unk>', 1e-8))\n",
    "        log_prob += math.log(prob)\n",
    "    return log_prob\n",
    "\n",
    "def perplexity(log_prob, n_tokens):\n",
    "    return math.exp(-log_prob / max(n_tokens, 1))\n",
    "\n",
    "results = []\n",
    "for s in sentences:\n",
    "    tokens = tokenize(s)\n",
    "    log_u = sentence_prob_unigram(tokens, unigram_probs)\n",
    "    p_u=math.exp(log_u)\n",
    "    log_b = sentence_prob_bigram(tokens, bigram_probs, unigram_probs)\n",
    "    p_b=math.exp(log_b)\n",
    "    n_tokens = len(tokens) + 1  # include </s>\n",
    "    results.append((s, log_u,p_u, log_b,p_b))\n",
    "\n",
    "# ---------- Save CSV ----------\n",
    "with open(out_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['sentence', 'logprob_unigram', 'prob_unigram', 'logprob_bigram','prob_bigram' ])\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Saved results to {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635f228-1bd8-4c1f-8586-34c2277d37b4",
   "metadata": {},
   "source": [
    "3️⃣ Why log-prob differs\n",
    "\n",
    "Context sensitivity:\n",
    "Unigram ignores word order → may assign moderate probability to unlikely sequences.\n",
    "Bigram considers order → likely sequences get higher log-prob, unlikely sequences get lower log-prob.\n",
    "\n",
    "Smoothing effects:\n",
    "Bigram smoothing often gives lower probabilities for unseen or rare sequences, so log-prob may be more negative than unigram in some cases.\n",
    "\n",
    "Sentence length impact:\n",
    "Longer sentences amplify differences because log-prob is additive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49cd99-8cfb-4335-9a5c-945426780c05",
   "metadata": {},
   "source": [
    "<b>more negative value of logprob refers rarer sentencescalc based on n-grams.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb307817-f13e-47c9-810c-4eb988fab15d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
